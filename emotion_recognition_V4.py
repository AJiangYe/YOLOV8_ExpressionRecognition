import gradio as gr
from ultralytics import YOLO
import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import matplotlib.font_manager as fm


# é…ç½®ä¸­æ–‡å­—ä½“
def configure_font():
    font_candidates = ["SimHei", "Microsoft YaHei", "Heiti TC", "WenQuanYi Micro Hei", "Arial Unicode MS"]
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    for font in font_candidates:
        if font in available_fonts:
            plt.rcParams["font.family"] = [font]
            return True
    print("è­¦å‘Š: æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå¯èƒ½å¯¼è‡´ä¸­æ–‡æ˜¾ç¤ºå¼‚å¸¸")
    return False


configure_font()

# æ¨¡å‹ä¸ç±»åˆ«é…ç½®
MODEL_PATH = "./runs/detect/train3/weights/best.pt"
EMOTION_CLASSES = ["angry", "disgust", "fear", "happy", "neutral", "sad", "surprise"]
EMOJI_MAP = {
    "angry": "ğŸ˜ ", "disgust": "ğŸ¤¢", "fear": "ğŸ˜¨",
    "happy": "ğŸ˜„", "neutral": "ğŸ˜", "sad": "ğŸ˜¢", "surprise": "ğŸ˜²"
}


class ROIOptimizedEmotionRecognizer:
    def __init__(self):
        self.model = YOLO(MODEL_PATH)
        if self.model.task != 'detect':
            raise RuntimeError("è¯·ä½¿ç”¨YOLOv8æ£€æµ‹æ¨¡å‹")
        self.class_id_map = {i: cls for i, cls in enumerate(EMOTION_CLASSES)}

        # ä¼˜åŒ–å‚æ•°
        self.face_detector = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        self.eye_detector = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_eye.xml'
        )
        self.min_face_size = (80, 80)  # æœ€å°äººè„¸å°ºå¯¸
        self.conf_threshold = 0.35  # ç½®ä¿¡åº¦é˜ˆå€¼
        self.nms_threshold = 0.45  # éæå¤§å€¼æŠ‘åˆ¶é˜ˆå€¼
        self.roi_expansion = 0.2  # åŒºåŸŸæ‰©å±•ç³»æ•°

    def precise_face_detection(self, image):
        """ç²¾ç¡®äººè„¸åŒºåŸŸæ£€æµ‹ï¼Œç»“åˆå¤šç‰¹å¾éªŒè¯"""
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

        # å¤šçº§äººè„¸æ£€æµ‹
        faces = self.face_detector.detectMultiScale(
            gray, scaleFactor=1.1, minNeighbors=8,
            minSize=self.min_face_size, flags=cv2.CASCADE_SCALE_IMAGE
        )

        if len(faces) == 0:
            return None

        # ç­›é€‰æœ€ä½³äººè„¸åŒºåŸŸï¼ˆæœ€å¤§ä¸”åŒ…å«çœ¼ç›ï¼‰
        best_face = None
        max_score = 0

        for (x, y, w, h) in faces:
            # æ‰©å±•äººè„¸åŒºåŸŸ
            h, w_img = image.shape[:2]
            x1 = max(0, int(x - w * self.roi_expansion))
            y1 = max(0, int(y - h * self.roi_expansion))
            x2 = min(w_img, int(x + w + w * self.roi_expansion))
            y2 = min(h, int(y + h + h * self.roi_expansion))
            face_roi = image[y1:y2, x1:x2]

            # çœ¼ç›æ£€æµ‹éªŒè¯
            roi_gray = cv2.cvtColor(face_roi, cv2.COLOR_RGB2GRAY)
            eyes = self.eye_detector.detectMultiScale(roi_gray, minSize=(20, 20))

            # è¯„åˆ†æœºåˆ¶ï¼šå¤§å°+çœ¼ç›æ•°é‡
            score = (w * h) + (len(eyes) * 1000)
            if score > max_score:
                max_score = score
                best_face = {
                    "roi": face_roi,
                    "original_coords": (x1, y1, x2, y2)
                }

        return best_face

    def preprocess_image(self, image):
        """ä¼˜åŒ–çš„å›¾åƒé¢„å¤„ç†æµç¨‹"""
        # 1. ç²¾ç¡®äººè„¸å®šä½
        face_data = self.precise_face_detection(image)
        if face_data is None:
            # å›é€€åˆ°é€šç”¨é¢„å¤„ç†
            processed_img = self.generic_preprocess(image)
            return processed_img, None
        else:
            # ä½¿ç”¨æ£€æµ‹åˆ°çš„äººè„¸åŒºåŸŸ
            processed_img = self.face_specific_preprocess(face_data["roi"])
            return processed_img, face_data["original_coords"]

    def face_specific_preprocess(self, face_roi):
        """é’ˆå¯¹äººè„¸åŒºåŸŸçš„é¢„å¤„ç†"""
        # ä¿æŒçºµæ¨ªæ¯”è°ƒæ•´å¤§å°
        h, w = face_roi.shape[:2]
        scale = min(640 / w, 640 / h)
        new_w, new_h = int(w * scale), int(h * scale)
        resized = cv2.resize(face_roi, (new_w, new_h), interpolation=cv2.INTER_AREA)

        # åˆ›å»ºå±…ä¸­ç”»å¸ƒ
        canvas = np.ones((640, 640, 3), dtype=np.uint8) * 255
        offset_x, offset_y = (640 - new_w) // 2, (640 - new_h) // 2
        canvas[offset_y:offset_y + new_h, offset_x:offset_x + new_w] = resized

        return cv2.cvtColor(canvas, cv2.COLOR_RGB2BGR)

    def generic_preprocess(self, image):
        """é€šç”¨å›¾åƒé¢„å¤„ç†ï¼ˆæ— æ£€æµ‹åˆ°äººè„¸æ—¶ä½¿ç”¨ï¼‰"""
        if len(image.shape) == 2:
            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
        elif image.shape[2] == 4:
            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)

        h, w = image.shape[:2]
        scale = min(640 / w, 640 / h)
        new_w, new_h = int(w * scale), int(h * scale)
        resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)

        canvas = np.ones((640, 640, 3), dtype=np.uint8) * 255
        offset_x, offset_y = (640 - new_w) // 2, (640 - new_h) // 2
        canvas[offset_y:offset_y + new_h, offset_x:offset_x + new_w] = resized

        return cv2.cvtColor(canvas, cv2.COLOR_RGB2BGR)

    def detect_emotion(self, image):
        if image is None:
            return None, None

        try:
            processed_img, face_coords = self.preprocess_image(image)
            results = self.model(
                processed_img,
                conf=self.conf_threshold,
                iou=self.nms_threshold,
                agnostic_nms=True
            )

            if len(results) == 0 or len(results[0].boxes) == 0:
                return None, face_coords

            boxes = results[0].boxes
            best_idx = np.argmax(boxes.conf.cpu().numpy())
            best_box = boxes[best_idx]

            return {
                "class_id": int(best_box.cls),
                "confidence": float(best_box.conf),
                "bbox": best_box.xyxy.cpu().numpy().tolist()[0],
                "class_name": self.class_id_map.get(int(best_box.cls), "unknown")
            }, face_coords
        except Exception as e:
            print(f"æ£€æµ‹é”™è¯¯: {str(e)}")
            return None, None

    def visualize_result(self, original_image, detection_result, face_coords):
        if detection_result is None:
            return "æœªæ£€æµ‹åˆ°æœ‰æ•ˆäººè„¸", "ğŸ¤·", None, None

        # ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒ
        class_id = detection_result["class_id"]
        confidence = detection_result["confidence"]
        probs = np.zeros(len(EMOTION_CLASSES))
        probs[class_id] = confidence
        remaining = (1.0 - confidence) / (len(EMOTION_CLASSES) - 1) if len(EMOTION_CLASSES) > 1 else 0
        for i in range(len(probs)):
            if i != class_id:
                probs[i] = remaining

        # ç”Ÿæˆæ¦‚ç‡ç›´æ–¹å›¾
        plt.figure(figsize=(10, 5))
        bars = plt.bar(EMOTION_CLASSES, probs, color='skyblue')
        bars[class_id].set_color('blue')
        plt.title("æƒ…ç»ªæ¦‚ç‡åˆ†å¸ƒ")
        plt.xlabel("æƒ…ç»ªç±»åˆ«")
        plt.ylabel("ç½®ä¿¡åº¦")
        plt.ylim(0, 1.0)
        plt.xticks(rotation=45)
        plt.tight_layout()
        hist_path = "emotion_histogram.png"
        plt.savefig(hist_path)
        plt.close()

        # ç»˜åˆ¶å¯è§†åŒ–ç»“æœ
        vis_image = original_image.copy()

        # ç»˜åˆ¶äººè„¸åŒºåŸŸæ¡†
        if face_coords:
            x1, y1, x2, y2 = face_coords
            cv2.rectangle(vis_image, (x1, y1), (x2, y2), (255, 165, 0), 2)  # æ©™è‰²æ¡†
            cv2.putText(
                vis_image, "äººè„¸åŒºåŸŸ", (x1, y1 - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 165, 0), 2
            )

        # ç»˜åˆ¶æƒ…ç»ªæ£€æµ‹æ¡†
        bbox = detection_result["bbox"]
        h, w = vis_image.shape[:2]
        x1, y1, x2, y2 = [int(coord) for coord in bbox]

        # è½¬æ¢åæ ‡åˆ°åŸå§‹å›¾åƒ
        scale = min(w / 640, h / 640)
        x1 = int(x1 * scale)
        y1 = int(y1 * scale)
        x2 = int(x2 * scale)
        y2 = int(y2 * scale)

        cv2.rectangle(vis_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
        label = f"{detection_result['class_name']}: {confidence:.2%}"
        cv2.putText(
            vis_image, label, (x1, y1 - 10),
            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2
        )

        vis_path = "detection_visualization.png"
        cv2.imwrite(vis_path, cv2.cvtColor(vis_image, cv2.COLOR_RGB2BGR))

        return (
            f"{detection_result['class_name']} (ç½®ä¿¡åº¦: {confidence:.2%})",
            EMOJI_MAP[detection_result['class_name']],
            vis_path,
            hist_path
        )

    def create_interface(self):
        with gr.Blocks(title="äººè„¸æƒ…ç»ªæ£€æµ‹ç³»ç»Ÿ_V4") as demo:
            gr.Markdown("""
            # ğŸ˜Š äººè„¸æƒ…ç»ªæ£€æµ‹ç³»ç»Ÿ
            ### ç²¾å‡†äººè„¸åŒºåŸŸå®šä½ä¸è¡¨æƒ…è¯†åˆ«
            """)
            with gr.Row():
                with gr.Column(scale=1):
                    input_image = gr.Image(type="numpy", label="ä¸Šä¼ å›¾åƒ")
                    detect_btn = gr.Button("å¼€å§‹æ£€æµ‹", variant="primary")
                    gr.Markdown("""
                    **ä¼˜åŒ–ç‰¹ç‚¹ï¼š**
                    - å¤šçº§äººè„¸æ£€æµ‹ä¸éªŒè¯
                    - çœ¼ç›ç‰¹å¾è¾…åŠ©å®šä½
                    - è‡ªé€‚åº”åŒºåŸŸæ‰©å±•
                    - éæå¤§å€¼æŠ‘åˆ¶å»é‡
                    """)

                with gr.Column(scale=2):
                    with gr.Row():
                        result_label = gr.Label(label="è¯†åˆ«ç»“æœ")
                        emoji_display = gr.Textbox(label="æƒ…ç»ªè¡¨æƒ…", interactive=False)
                    with gr.Row():
                        with gr.Column():
                            detection_vis = gr.Image(label="æ£€æµ‹å¯è§†åŒ–", type="filepath")
                        with gr.Column():
                            probability_hist = gr.Image(label="æƒ…ç»ªæ¦‚ç‡åˆ†å¸ƒ", type="filepath")

            def process_image(image):
                if image is None:
                    return "è¯·ä¸Šä¼ å›¾åƒ", "â“", None, None

                try:
                    detection_result, face_coords = self.detect_emotion(image)
                    result_text, emoji, vis_path, hist_path = self.visualize_result(
                        image, detection_result, face_coords
                    )
                    return result_text, emoji, vis_path, hist_path
                except Exception as e:
                    return f"å¤„ç†é”™è¯¯: {str(e)}", "ğŸ’¥", None, None

            detect_btn.click(
                fn=process_image,
                inputs=input_image,
                outputs=[result_label, emoji_display, detection_vis, probability_hist]
            )

            input_image.change(
                fn=process_image,
                inputs=input_image,
                outputs=[result_label, emoji_display, detection_vis, probability_hist]
            )

        return demo


if __name__ == "__main__":
    try:
        app = ROIOptimizedEmotionRecognizer()
        demo = app.create_interface()
        demo.launch(server_name="127.0.0.1", server_port=7860, share=False)
    except Exception as e:
        print(f"åº”ç”¨å¯åŠ¨å¤±è´¥: {str(e)}")