import gradio as gr
from ultralytics import YOLO
import cv2
import numpy as np
import matplotlib.pyplot as plt
import os

# é…ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams["font.family"] = ["SimHei", "SimHei", "SimHei"]

# æ¨¡å‹ä¸ç±»åˆ«é…ç½®ï¼ˆé€‚é…æ£€æµ‹æ¨¡å‹ï¼‰
MODEL_PATH = "./runs/detect/train3/weights/best.pt"  # æ£€æµ‹æ¨¡å‹è·¯å¾„
EMOTION_CLASSES = ["angry", "disgust", "fear", "happy", "neutral", "sad", "surprise"]  # 7ç±»æƒ…ç»ª
EMOJI_MAP = {
    "angry": "ğŸ˜ ", "disgust": "ğŸ¤¢", "fear": "ğŸ˜¨",
    "happy": "ğŸ˜„", "neutral": "ğŸ˜", "sad": "ğŸ˜¢", "surprise": "ğŸ˜²"
}


class DetectionToClassificationAdapter:
    def __init__(self):
        # åŠ è½½æ£€æµ‹æ¨¡å‹
        self.model = YOLO(MODEL_PATH)
        # éªŒè¯æ¨¡å‹ç±»å‹
        if self.model.task != 'detect':
            raise RuntimeError("è¯·ä½¿ç”¨YOLOv8æ£€æµ‹æ¨¡å‹")

        # ç±»åˆ«IDæ˜ å°„ï¼ˆç¡®ä¿ä¸æ£€æµ‹æ¨¡å‹çš„classes.txtä¸€è‡´ï¼‰
        self.class_id_map = {i: cls for i, cls in enumerate(EMOTION_CLASSES)}

    def detect_emotion(self, image):
        """ä½¿ç”¨æ£€æµ‹æ¨¡å‹è¿›è¡Œæƒ…ç»ªè¯†åˆ«ï¼ˆæå–ç½®ä¿¡åº¦æœ€é«˜çš„ç›®æ ‡ï¼‰"""
        if image is None:
            return None

        # è½¬æ¢ä¸ºBGRæ ¼å¼ï¼ˆYOLOv8é»˜è®¤è¾“å…¥ï¼‰
        if len(image.shape) == 3 and image.shape[2] == 3:
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

        # æ¨¡å‹æ¨ç†ï¼ˆæ£€æµ‹äººè„¸æƒ…ç»ªåŒºåŸŸï¼‰
        results = self.model(image, conf=0.3)  # ç½®ä¿¡åº¦é˜ˆå€¼0.3

        # æå–ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹ç»“æœ
        if len(results) == 0 or len(results[0].boxes) == 0:
            return None

        # è·å–æœ€ä½³æ£€æµ‹æ¡†
        boxes = results[0].boxes
        best_idx = np.argmax(boxes.conf.cpu().numpy())  # æœ€é«˜ç½®ä¿¡åº¦ç´¢å¼•
        best_box = boxes[best_idx]

        return {
            "class_id": int(best_box.cls),
            "confidence": float(best_box.conf),
            "bbox": best_box.xyxy.cpu().numpy().tolist()[0],  # [x1,y1,x2,y2]
            "class_name": self.class_id_map.get(int(best_box.cls), "unknown")
        }

    def visualize_result(self, image, detection_result):
        """å¯è§†åŒ–æ£€æµ‹ç»“æœå’Œæƒ…ç»ªæ¦‚ç‡"""
        if detection_result is None:
            return "æœªæ£€æµ‹åˆ°æƒ…ç»ªåŒºåŸŸ", "ğŸ¤·", None

        # åˆ›å»ºæƒ…ç»ªæ¦‚ç‡åˆ†å¸ƒï¼ˆæ¨¡æ‹Ÿåˆ†ç±»æ¨¡å‹è¾“å‡ºï¼‰
        # æ£€æµ‹æ¨¡å‹åªèƒ½æä¾›å•ä¸ªç±»åˆ«ç½®ä¿¡åº¦ï¼Œè¿™é‡Œå°†å…¶è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        class_id = detection_result["class_id"]
        confidence = detection_result["confidence"]
        probs = np.zeros(len(EMOTION_CLASSES))
        probs[class_id] = confidence
        # åˆ†é…å‰©ä½™æ¦‚ç‡ç»™å…¶ä»–ç±»åˆ«ï¼ˆæ¨¡æ‹Ÿåˆ†å¸ƒï¼‰
        remaining = (1.0 - confidence) / (len(EMOTION_CLASSES) - 1)
        for i in range(len(probs)):
            if i != class_id:
                probs[i] = remaining

        # ç”Ÿæˆæ¦‚ç‡ç›´æ–¹å›¾
        plt.figure(figsize=(10, 5))
        bars = plt.bar(EMOTION_CLASSES, probs, color='skyblue')
        bars[class_id].set_color('blue')  # é«˜äº®æ£€æµ‹ç±»åˆ«
        plt.title("æƒ…ç»ªæ¦‚ç‡åˆ†å¸ƒï¼ˆæ£€æµ‹æ¨¡å‹æ¨¡æ‹Ÿï¼‰")
        plt.xlabel("æƒ…ç»ªç±»åˆ«")
        plt.ylabel("ç½®ä¿¡åº¦")
        plt.ylim(0, 1.0)
        plt.xticks(rotation=45)
        plt.tight_layout()
        hist_path = "emotion_detection_histogram.png"
        plt.savefig(hist_path)
        plt.close()

        # ç»˜åˆ¶å¸¦è¾¹ç•Œæ¡†çš„å›¾åƒ
        vis_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) if len(image.shape) == 3 else image
        bbox = detection_result["bbox"]
        x1, y1, x2, y2 = map(int, bbox)
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        cv2.rectangle(vis_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
        # æ·»åŠ ç±»åˆ«æ ‡ç­¾
        label = f"{detection_result['class_name']}: {confidence:.2%}"
        cv2.putText(
            vis_image, label, (x1, y1 - 10),
            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2
        )
        # ä¿å­˜å¯è§†åŒ–ç»“æœ
        vis_path = "detection_visualization.png"
        cv2.imwrite(vis_path, vis_image)

        return (
            f"{detection_result['class_name']} (ç½®ä¿¡åº¦: {confidence:.2%})",
            EMOJI_MAP[detection_result['class_name']],
            hist_path
        )

    def create_interface(self):
        """åˆ›å»ºGradioç•Œé¢"""
        with gr.Blocks(title="äººè„¸æƒ…ç»ªæ£€æµ‹ç³»ç»Ÿ") as demo:
            gr.Markdown("""
            # ğŸ˜Š äººè„¸æƒ…ç»ªæ£€æµ‹ç³»ç»Ÿ
            ### åŸºäºYOLOv8æ£€æµ‹æ¨¡å‹çš„æƒ…ç»ªè¯†åˆ«
            """)
            with gr.Row():
                with gr.Column(scale=1):
                    input_image = gr.Image(type="numpy", label="ä¸Šä¼ å›¾åƒ")
                    detect_btn = gr.Button("å¼€å§‹æ£€æµ‹", variant="primary")

                with gr.Column(scale=2):
                    with gr.Row():
                        result_label = gr.Label(label="æ£€æµ‹ç»“æœ")
                        emoji_display = gr.Textbox(label="æƒ…ç»ªè¡¨æƒ…", interactive=False)
                    with gr.Row():
                        with gr.Column():
                            detection_vis = gr.Image(label="æ£€æµ‹å¯è§†åŒ–", type="filepath")
                        with gr.Column():
                            probability_hist = gr.Image(label="æƒ…ç»ªæ¦‚ç‡åˆ†å¸ƒ", type="filepath")

            def process_image(image):
                if image is None:
                    return "è¯·ä¸Šä¼ å›¾åƒ", "â“", None, None

                try:
                    # æ£€æµ‹æƒ…ç»ªåŒºåŸŸ
                    detection_result = self.detect_emotion(image)
                    # å¯è§†åŒ–ç»“æœ
                    result_text, emoji, hist_path = self.visualize_result(image, detection_result)
                    # è¿”å›æ£€æµ‹å¯è§†åŒ–å›¾åƒ
                    return result_text, emoji, "detection_visualization.png", hist_path
                except Exception as e:
                    return f"å¤„ç†é”™è¯¯: {str(e)}", "ğŸ’¥", None, None

            detect_btn.click(
                fn=process_image,
                inputs=input_image,
                outputs=[result_label, emoji_display, detection_vis, probability_hist]
            )

            input_image.change(
                fn=process_image,
                inputs=input_image,
                outputs=[result_label, emoji_display, detection_vis, probability_hist]
            )

        return demo


if __name__ == "__main__":
    try:
        app = DetectionToClassificationAdapter()
        demo = app.create_interface()
        demo.launch(server_name="127.0.0.1", server_port=7860)
    except Exception as e:
        print(f"åº”ç”¨å¯åŠ¨å¤±è´¥: {str(e)}")