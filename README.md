# YOLOV8_ExpressionRecognition
åŸºäºYOLOV8å¼€å‘çš„è½»é‡åŒ–æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿçš„ç ”ç©¶ä¸å¼€å‘
[äººè„¸æƒ…ç»ªè¯†åˆ«é¡¹ç›®ä½¿ç”¨è¯´æ˜ä¹¦.md](https://github.com/user-attachments/files/23904973/default.md)
# åŸºäºYOLOV8çš„äººè„¸æƒ…ç»ªè¯†åˆ«ä½¿ç”¨è¯´æ˜

## ğŸ“Œ é¡¹ç›®ç®€ä»‹ 

æœ¬é¡¹ç›®åŸºäº **YOLOv8** è¿›è¡Œ **äººè„¸è¡¨æƒ…è¯†åˆ«**ï¼Œä» **é¢éƒ¨è¡¨æƒ…æ•°æ®é›†** è®­ç»ƒä¸€ä¸ªç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œè¯†åˆ« **æ„¤æ€’ (angry)ã€åŒæ¶ (disgust)ã€ææƒ§ (fear)ã€é«˜å…´ (happy)ã€ä¸­æ€§ (neutral)ã€æ‚²ä¼¤ (sad)ã€æƒŠè®¶ (surprise)** ä¸ƒç§æƒ…ç»ªã€‚

åæ”¹è¿›æ·»åŠ ç”¨æˆ·äº¤äº’å¤šç•Œé¢ï¼ˆGUIã€Webï¼‰ï¼Œå®ç°**äººæœºäº¤äº’**å°é¡¹ç›®ã€‚

------

## ğŸ¯å¿«é€Ÿå¼€å§‹

ç”±äºæœ¬é¡¹ç›®å·²ç»è®­ç»ƒå¥½äº†æ¨¡å‹ï¼Œæ•…æŠŠå¿«é€Ÿå¼€å§‹æ–‡ä»¶å†™åœ¨å¼€å¤´ï¼š

æ‰“å¼€é¡¹ç›®æ–‡ä»¶å¤¹åï¼Œå…ˆåˆ›å»ºcondaè™šæ‹Ÿç¯å¢ƒï¼Œè¿™é‡Œæˆ‘ç”¨çš„python=3.9ç¯å¢ƒ

å†è¿è¡Œrequirement.txtæ–‡ä»¶ï¼Œ

`pip install requirement.txt` 

è¿›å…¥è¿è¡Œæ–‡ä»¶å¤¹

$cd ./YOLOV8_ExpressionRecognition/working $ 

è¿è¡Œæµ‹è¯•pyæ–‡ä»¶

`pyton emotion_recognition_V4.py`     #æ‰“å¼€gradioç•Œé¢

`python GUI.py`     #æ‰“å¼€GUIç•Œé¢

## ğŸ“ æ•°æ®é›†

æœ¬é¡¹ç›®ä½¿ç”¨ **face-expression-recognition-dataset**ï¼Œè¯¥æ•°æ®é›†å·²æŒ‰ç…§è¡¨æƒ…ç±»åˆ«å­˜æ”¾åœ¨ä¸åŒçš„æ–‡ä»¶å¤¹ï¼š

```bash
face-expression-recognition-dataset/
â”‚â”€â”€ images/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ angry/
â”‚   â”‚   â”œâ”€â”€ disgust/
â”‚   â”‚   â”œâ”€â”€ fear/
â”‚   â”‚   â”œâ”€â”€ happy/
â”‚   â”‚   â”œâ”€â”€ neutral/
â”‚   â”‚   â”œâ”€â”€ sad/
â”‚   â”‚   â”œâ”€â”€ surprise/
â”‚   â”œâ”€â”€ validation/
â”‚   â”‚   â”œâ”€â”€ angry/
â”‚   â”‚   â”œâ”€â”€ disgust/
â”‚   â”‚   â”œâ”€â”€ fear/
â”‚   â”‚   â”œâ”€â”€ happy/
â”‚   â”‚   â”œâ”€â”€ neutral/
â”‚   â”‚   â”œâ”€â”€ sad/
â”‚   â”‚   â”œâ”€â”€ surprise/
```

æ•°æ®é›†åŒ…å« **è®­ç»ƒé›† (train) å’Œ éªŒè¯é›† (validation)**ã€‚ 

------

## ğŸš€ æ¨¡å‹é€‰æ‹© 

æœ¬é¡¹ç›®é‡‡ç”¨ **YOLOv8 (You Only Look Once v8)** è¿›è¡Œè¡¨æƒ…æ£€æµ‹ã€‚**ä¸ºä»€ä¹ˆé€‰æ‹© YOLOv8?**

âœ… **é«˜æ•ˆæ€§ (Efficiency)** - å…·å¤‡å®æ—¶ç›®æ ‡æ£€æµ‹èƒ½åŠ›ã€‚

 âœ… **ç²¾å‡†åº¦ (Accuracy)** - é€‚ç”¨äºå°ç›®æ ‡æ£€æµ‹ï¼Œå¦‚é¢éƒ¨è¡¨æƒ…ã€‚ 

âœ… **ç«¯åˆ°ç«¯è®­ç»ƒ (End-to-end training)** - ç›´æ¥ä»æ•°æ®é›†ä¸­å­¦ä¹ ï¼Œä¸éœ€è¦é¢å¤–ç‰¹å¾å·¥ç¨‹ã€‚

------

## ğŸ”„ æ•°æ®é¢„å¤„ç† 

ç”±äºåŸå§‹æ•°æ®ä¸ºåˆ†ç±»æ ¼å¼ï¼ŒYOLO éœ€è¦ **ç›®æ ‡æ£€æµ‹æ ¼å¼** (bounding box æ ‡ç­¾)ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä»¥ä¸‹è½¬æ¢ï¼š

Since the original dataset is in classification format, YOLO requires **object detection format** (bounding box labels). We performed the following conversions:

1. **ä½¿ç”¨ OpenCV è¿›è¡Œäººè„¸æ£€æµ‹** (Use OpenCV for face detection)
2. **ç”Ÿæˆ YOLO æ ¼å¼æ ‡ç­¾** (Generate YOLO format labels)
3. **é‡æ–°ç»„ç»‡æ•°æ®é›†ç»“æ„** (Reorganize dataset structure)

è½¬æ¢åçš„æ•°æ®æ ¼å¼å¦‚ä¸‹ (The converted dataset format is as follows):

```bash
yolo_dataset/
â”‚â”€â”€ images/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚â”€â”€ labels/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
```

------

## ğŸ¯ è®­ç»ƒ YOLOv8

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è®­ç»ƒ YOLOv8: Train YOLOv8 using the following command:

```python
from ultralytics import YOLO

# åŠ è½½ YOLOv8 é¢„è®­ç»ƒæ¨¡å‹ 
model = YOLO("yolov8n.yaml")

# è®­ç»ƒæ¨¡å‹ (Train the model)
results = model.train(
    data="./data.yaml",  # æ³¨æ„æ”¹æˆè‡ªå·±çš„æ•°æ®é›†è·¯å¾„ 
    epochs=50,  # è®­ç»ƒè½®æ•° 
    imgsz=480,  # å›¾åƒå°ºå¯¸ 
    batch=16,  # æ‰¹é‡å¤§å° 
    device="cuda" if torch.cuda.is_available() else "cpu"
)
```

è®­ç»ƒç»“æœä¼šä¿å­˜åœ¨ `runs/detect/trainX/` ç›®å½•ä¸‹ã€‚ 

------

## ğŸ“Š è®­ç»ƒç»“æœå¯è§†åŒ–

### è®­ç»ƒæŸå¤± & mAP æ›²çº¿

```python
import matplotlib.pyplot as plt
import os

train_results_dir = "runs/detect/train3"  # æ›¿æ¢ä¸ºä½ çš„è®­ç»ƒç›®å½•ï¼Œè¿™é‡Œæˆ‘æ˜¯è¿è¡Œäº†ä¸‰æ¬¡ï¼Œæ‰€ä»¥å­˜åœ¨äº†rain3ä¸­

metrics = ["results.png", "F1_curve.png", "PR_curve.png", "confusion_matrix.png"]
plt.figure(figsize=(12, 6))

for i, metric in enumerate(metrics):
    metric_path = os.path.join(train_results_dir, metric)
    if os.path.exists(metric_path):
        img = plt.imread(metric_path)
        plt.subplot(2, 2, i + 1)
        plt.imshow(img)
        plt.axis("off")
        plt.title(metric.replace(".png", ""))

plt.tight_layout()
plt.show()
```

------

## ğŸ­ è¿è¡Œæ¨ç† 

```python
from ultralytics import YOLO

model = YOLO("runs/detect/train3/weights/best.pt")  # è½½å…¥æœ€ä¼˜æ¨¡å‹ 
results = model.predict("/kaggle/working/test_image.jpg", save=True)
```

å¯è§†åŒ–æ£€æµ‹ç»“æœ (Visualize detection results):

```python
import cv2
import matplotlib.pyplot as plt

output_image_path = results[0].save_dir + "/test_image.jpg"
output_image = cv2.imread(output_image_path)
output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)

plt.figure(figsize=(8, 6))
plt.imshow(output_image)
plt.axis("off")
plt.title("YOLOv8 è¡¨æƒ…æ£€æµ‹ç»“æœ (YOLOv8 Facial Expression Detection Result)")
plt.show()
```

------

## ğŸ”š ç»“è®º (Conclusion)

âœ… **æˆåŠŸè®­ç»ƒ YOLOv8 è¿›è¡Œäººè„¸è¡¨æƒ…æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿçš„ç ”ç©¶ä¸å¼€å‘** 

âœ… **å®ç°äº†æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€ç»“æœå¯è§†åŒ–å’Œæ¨ç†**

âœ… **æœªæ¥æ”¹è¿›æ–¹å‘ï¼šä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹ã€å¢åŠ æ•°æ®å¢å¼ºã€æé«˜ mAP** 
