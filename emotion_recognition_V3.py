import gradio as gr
from ultralytics import YOLO
import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import matplotlib.font_manager as fm


# é…ç½®ä¸­æ–‡å­—ä½“
def configure_font():
    font_candidates = ["SimHei", "Microsoft YaHei", "Heiti TC", "WenQuanYi Micro Hei", "Arial Unicode MS"]
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    for font in font_candidates:
        if font in available_fonts:
            plt.rcParams["font.family"] = [font]
            return True
    print("è­¦å‘Š: æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå¯èƒ½å¯¼è‡´ä¸­æ–‡æ˜¾ç¤ºå¼‚å¸¸")
    return False


configure_font()

# æ¨¡å‹ä¸ç±»åˆ«é…ç½®
MODEL_PATH = "./runs/detect/train3/weights/best.pt"
EMOTION_CLASSES = ["angry", "disgust", "fear", "happy", "neutral", "sad", "surprise"]
EMOJI_MAP = {
    "angry": "ğŸ˜ ", "disgust": "ğŸ¤¢", "fear": "ğŸ˜¨",
    "happy": "ğŸ˜„", "neutral": "ğŸ˜", "sad": "ğŸ˜¢", "surprise": "ğŸ˜²"
}


class ImageProcessingEmotionRecognizer:
    def __init__(self):
        self.model = YOLO(MODEL_PATH)
        if self.model.task != 'detect':
            raise RuntimeError("è¯·ä½¿ç”¨YOLOv8æ£€æµ‹æ¨¡å‹")
        self.class_id_map = {i: cls for i, cls in enumerate(EMOTION_CLASSES)}

    def preprocess_image(self, image):
        """å¢å¼ºå‹å›¾åƒé¢„å¤„ç†ï¼Œæ”¯æŒä»»æ„å½©è‰²å›¾ç‰‡è¾“å…¥"""
        # 1. ç¡®ä¿å›¾åƒä¸º3é€šé“RGBæ ¼å¼
        if len(image.shape) == 2:  # ç°åº¦å›¾è½¬å½©è‰²
            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
        elif image.shape[2] == 4:  # RGBAè½¬RGB
            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)

        # 2. ç»Ÿä¸€è°ƒæ•´ä¸ºæ¨¡å‹æ¨èå°ºå¯¸(640x640)ï¼Œä¿æŒçºµæ¨ªæ¯”
        h, w = image.shape[:2]
        scale = min(640 / w, 640 / h)
        new_w, new_h = int(w * scale), int(h * scale)
        resized = cv2.resize(image, (new_w, new_h))

        # 3. åˆ›å»ºç©ºç™½ç”»å¸ƒå¹¶å±…ä¸­æ”¾ç½®å›¾åƒï¼ˆé¿å…ç•¸å˜ï¼‰
        canvas = np.ones((640, 640, 3), dtype=np.uint8) * 255  # ç™½è‰²èƒŒæ™¯
        offset_x, offset_y = (640 - new_w) // 2, (640 - new_h) // 2
        canvas[offset_y:offset_y + new_h, offset_x:offset_x + new_w] = resized

        # 4. è½¬æ¢ä¸ºYOLOv8è¦æ±‚çš„BGRæ ¼å¼
        return cv2.cvtColor(canvas, cv2.COLOR_RGB2BGR)

    def detect_emotion(self, image):
        if image is None:
            return None

        try:
            # å¢å¼ºå‹é¢„å¤„ç†
            processed_img = self.preprocess_image(image)

            # æ¨¡å‹æ¨ç†
            results = self.model(processed_img, conf=0.3)
            if len(results) == 0 or len(results[0].boxes) == 0:
                return None

            # è·å–æœ€ä½³æ£€æµ‹ç»“æœ
            boxes = results[0].boxes
            best_idx = np.argmax(boxes.conf.cpu().numpy())
            best_box = boxes[best_idx]

            return {
                "class_id": int(best_box.cls),
                "confidence": float(best_box.conf),
                "bbox": best_box.xyxy.cpu().numpy().tolist()[0],
                "class_name": self.class_id_map.get(int(best_box.cls), "unknown")
            }
        except Exception as e:
            print(f"æ£€æµ‹è¿‡ç¨‹å‡ºé”™: {str(e)}")
            return None

    def visualize_result(self, original_image, detection_result):
        if detection_result is None:
            return "æœªæ£€æµ‹åˆ°æƒ…ç»ªåŒºåŸŸ", "ğŸ¤·", None, None

        # ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒ
        class_id = detection_result["class_id"]
        confidence = detection_result["confidence"]
        probs = np.zeros(len(EMOTION_CLASSES))
        probs[class_id] = confidence
        remaining = (1.0 - confidence) / (len(EMOTION_CLASSES) - 1) if len(EMOTION_CLASSES) > 1 else 0
        for i in range(len(probs)):
            if i != class_id:
                probs[i] = remaining

        # ç”Ÿæˆæ¦‚ç‡ç›´æ–¹å›¾
        plt.figure(figsize=(10, 5))
        bars = plt.bar(EMOTION_CLASSES, probs, color='skyblue')
        bars[class_id].set_color('blue')
        plt.title("æƒ…ç»ªæ¦‚ç‡åˆ†å¸ƒ")
        plt.xlabel("æƒ…ç»ªç±»åˆ«")
        plt.ylabel("ç½®ä¿¡åº¦")
        plt.ylim(0, 1.0)
        plt.xticks(rotation=45)
        plt.tight_layout()
        hist_path = "emotion_histogram.png"
        plt.savefig(hist_path)
        plt.close()

        # ç»˜åˆ¶å¸¦è¾¹ç•Œæ¡†çš„åŸå§‹å›¾åƒ
        vis_image = original_image.copy()
        bbox = detection_result["bbox"]
        # å°†æ¨¡å‹è¾“å…¥åæ ‡è½¬æ¢å›åŸå§‹å›¾åƒåæ ‡
        h, w = original_image.shape[:2]
        scale = min(640 / w, 640 / h)
        offset_x, offset_y = (640 - int(w * scale)) // 2, (640 - int(h * scale)) // 2

        # è¿˜åŸè¾¹ç•Œæ¡†åˆ°åŸå§‹å›¾åƒ
        x1, y1, x2, y2 = bbox
        x1 = int((x1 - offset_x) / scale)
        y1 = int((y1 - offset_y) / scale)
        x2 = int((x2 - offset_x) / scale)
        y2 = int((y2 - offset_y) / scale)

        # ç»˜åˆ¶è¾¹ç•Œæ¡†å’Œæ ‡ç­¾
        cv2.rectangle(vis_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
        label = f"{detection_result['class_name']}: {confidence:.2%}"
        cv2.putText(
            vis_image, label, (x1, y1 - 10),
            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2
        )

        # ä¿å­˜å¯è§†åŒ–ç»“æœ
        vis_path = "detection_visualization.png"
        cv2.imwrite(vis_path, cv2.cvtColor(vis_image, cv2.COLOR_RGB2BGR))

        return (
            f"{detection_result['class_name']} (ç½®ä¿¡åº¦: {confidence:.2%})",
            EMOJI_MAP[detection_result['class_name']],
            vis_path,
            hist_path
        )

    def create_interface(self):
        with gr.Blocks(title="äººè„¸æƒ…ç»ªæ£€æµ‹ç³»ç»Ÿ") as demo:
            gr.Markdown("""
            # ğŸ˜Š äººè„¸æƒ…ç»ªæ£€æµ‹ç³»ç»Ÿ
            ### æ”¯æŒä»»æ„å½©è‰²å›¾ç‰‡è¾“å…¥çš„æƒ…ç»ªè¯†åˆ«
            """)
            with gr.Row():
                with gr.Column(scale=1):
                    input_image = gr.Image(type="numpy", label="ä¸Šä¼ å›¾åƒï¼ˆæ”¯æŒä»»æ„æ ¼å¼ï¼‰")
                    detect_btn = gr.Button("å¼€å§‹æ£€æµ‹", variant="primary")

                with gr.Column(scale=2):
                    with gr.Row():
                        result_label = gr.Label(label="æ£€æµ‹ç»“æœ")
                        emoji_display = gr.Textbox(label="æƒ…ç»ªè¡¨æƒ…", interactive=False)
                    with gr.Row():
                        with gr.Column():
                            detection_vis = gr.Image(label="æ£€æµ‹å¯è§†åŒ–", type="filepath")
                        with gr.Column():
                            probability_hist = gr.Image(label="æƒ…ç»ªæ¦‚ç‡åˆ†å¸ƒ", type="filepath")

            def process_image(image):
                if image is None:
                    return "è¯·ä¸Šä¼ å›¾åƒ", "â“", None, None

                try:
                    detection_result = self.detect_emotion(image)
                    result_text, emoji, vis_path, hist_path = self.visualize_result(image, detection_result)
                    return result_text, emoji, vis_path, hist_path
                except Exception as e:
                    return f"å¤„ç†é”™è¯¯: {str(e)}", "ğŸ’¥", None, None

            detect_btn.click(
                fn=process_image,
                inputs=input_image,
                outputs=[result_label, emoji_display, detection_vis, probability_hist]
            )

            input_image.change(
                fn=process_image,
                inputs=input_image,
                outputs=[result_label, emoji_display, detection_vis, probability_hist]
            )

        return demo


if __name__ == "__main__":
    try:
        app = ImageProcessingEmotionRecognizer()
        demo = app.create_interface()
        demo.launch(server_name="127.0.0.1", server_port=7860, share=False)
    except Exception as e:
        print(f"åº”ç”¨å¯åŠ¨å¤±è´¥: {str(e)}")